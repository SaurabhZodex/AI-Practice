{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical or Traditional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Corpus : ['geeks for geeks', 'geeks learning together fast', 'nlp ohe']\n",
      "Our vocabulary :  {'geeks': 1, 'for': 2, 'learning': 3, 'together': 4, 'fast': 5, 'nlp': 6, 'ohe': 7}\n",
      "\n",
      "OneHotEncoded vector for sentence : \" geeks for geeks \"is \n",
      " [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Dataset :  [[[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0]], [[1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0]], [[0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# nltk.download('punkt') # Download 'punkt' \n",
    "# from nltk if it's not downloaded \n",
    "from nltk.tokenize import sent_tokenize \n",
    "Text = \"\"\"Geeks For Geeks. \n",
    "\t\tGeeks Learning Together Fast.\n",
    "        NLP OHE.\"\"\"\n",
    "sentences = sent_tokenize(Text) \n",
    "sentences = [sent.lower().replace(\".\", \"\") for sent in sentences] \n",
    "print('Our Corpus :', sentences) \n",
    "\n",
    "# Create the vocabulary \n",
    "vocab = {}\n",
    "corpus = \" \".join(sentences)\n",
    "count = 0\n",
    "for word in corpus.split():\n",
    "    if word not in vocab:\n",
    "        count += 1\n",
    "        vocab[word] = count\n",
    "print(\"Our vocabulary : \", vocab)\n",
    "\n",
    "# One Hot Encoding \n",
    "def OneHotEncoder(text): \n",
    "\tonehot_encoded = [] \n",
    "\tfor word in text.split(): \n",
    "\t\ttemp = [0]*len(vocab) \n",
    "\t\tif word in vocab: \n",
    "\t\t\ttemp[vocab[word]-1] = 1\n",
    "\t\t\tonehot_encoded.append(temp) \n",
    "\treturn onehot_encoded \n",
    "\n",
    "\n",
    "print('\\nOneHotEncoded vector for sentence : \"', \n",
    "\tsentences[0], '\"is \\n', OneHotEncoder(sentences[0]))\n",
    "\n",
    "# Creating dataset\n",
    "dataset = []\n",
    "for sentence in sentences:\n",
    "    dataset.append(OneHotEncoder(sentence))\n",
    "\n",
    "print(\"\\nDataset : \", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Corpus: ['geeks for geeks', 'geeks learning together fast', 'nlp ohe']\n",
      "\n",
      "Our vocabulary:  {'geeks': 2, 'for': 1, 'learning': 3, 'together': 6, 'fast': 0, 'nlp': 4, 'ohe': 5}\n",
      "\n",
      "BoW representation for geeks for geeks [[0 1 2 0 0 0 0]]\n",
      "BoW representation for geeks learning together fast [[1 0 1 1 0 0 1]]\n",
      "BoW representation for nlp ohe [[0 0 0 0 1 1 0]]\n",
      "\n",
      "Bow representation for 'learning dsa from AN': [[0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "#nltk.download('punkt') # Download 'punkt' from nltk if it's not downloaded \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "Text = \"\"\"Geeks For Geeks. \n",
    "\t\tGeeks Learning Together Fast.\n",
    "        NLP OHE.\"\"\"\n",
    "# TOKENIZATION \n",
    "sentences = sent_tokenize(Text) \n",
    "sentences = [sent.lower().replace(\".\",\"\") for sent in sentences] \n",
    "print('\\nOur Corpus:',sentences)\n",
    "\n",
    "#CountVectorizer : Convert a collection of text documents to a matrix of token counts. \n",
    "count_vect = CountVectorizer() \n",
    "# fit & transform will represent each sentences as BOW representation \n",
    "BOW = count_vect.fit_transform(sentences) \n",
    "# Get the vocabulary \n",
    "print(\"\\nOur vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW representation \n",
    "print(f\"\\nBoW representation for {sentences[0]} {BOW[0].toarray()}\") \n",
    "print(f\"BoW representation for {sentences[1]} {BOW[1].toarray()}\") \n",
    "print(f\"BoW representation for {sentences[2]} {BOW[2].toarray()}\") \n",
    "# BOW representation for a new text \n",
    "BOW_ = count_vect.transform([\"learning dsa from AN\"]) \n",
    "print(\"\\nBow representation for 'learning dsa from AN':\", BOW_.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Corpus: ['geeks for geeks', 'geeks learning together fast', 'nlp ohe']\n",
      "\n",
      "Our vocabulary: {'geeks': 3, 'for': 1, 'geeks for': 4, 'for geeks': 2, 'geeks for geeks': 5, 'learning': 8, 'together': 14, 'fast': 0, 'geeks learning': 6, 'learning together': 9, 'together fast': 15, 'geeks learning together': 7, 'learning together fast': 10, 'nlp': 11, 'ohe': 13, 'nlp ohe': 12}\n",
      "\n",
      "Ngram representation for \"geeks for geeks\" is [[0 1 1 2 1 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "Ngram representation for \"geeks learning together fast\" is [[1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1]]\n",
      "Ngram representation for \"nlp ohe\" is [[0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0]]\n",
      "\n",
      "Ngram representation for 'learning dsa from AN' is [[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# nltk.download('punkt') # Download 'punkt' \n",
    "# from nltk if it's not downloaded \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "Text = \"\"\"Geeks For Geeks. \n",
    "\t\tGeeks Learning Together Fast.\n",
    "        NLP OHE.\"\"\"\n",
    "\n",
    "# TOKENIZATION \n",
    "sentences = sent_tokenize(Text) \n",
    "sentences = [sent.lower().replace(\".\", \"\") for sent in sentences] \n",
    "print('\\nOur Corpus:', sentences) \n",
    "\n",
    "# Ngram vectorization example with count \n",
    "# vectorizer and uni, bi, trigrams \n",
    "count_vect = CountVectorizer(ngram_range=(1, 3)) \n",
    "\n",
    "# fit & transform will represent each sentences \n",
    "# as Bag of n-grams representation \n",
    "BOW_nGram = count_vect.fit_transform(sentences) \n",
    "\n",
    "# Get the vocabulary \n",
    "print(\"\\nOur vocabulary:\", count_vect.vocabulary_) \n",
    "\n",
    "# see the Bag of n-grams representation \n",
    "print('\\nNgram representation for \"{}\" is {}'\n",
    "\t.format(sentences[0], BOW_nGram[0].toarray())) \n",
    "print('Ngram representation for \"{}\" is {}'\n",
    "\t.format(sentences[1], BOW_nGram[1].toarray())) \n",
    "print('Ngram representation for \"{}\" is {}'. \n",
    "\tformat(sentences[2], BOW_nGram[2].toarray())) \n",
    "\n",
    "# Bag of n-grams representation for a new text \n",
    "BOW_nGram_ = count_vect.transform([\"learning dsa from AN\"]) \n",
    "print(\"\\nNgram representation for 'learning dsa from AN' is\", \n",
    "\tBOW_nGram_.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Corpus: ['geeks for geeks', 'geeks learning together fast', 'nlp ohe']\n",
      "\n",
      "vocabulary ['fast' 'for' 'geeks' 'learning' 'nlp' 'ohe' 'together']\n",
      "\n",
      "IDF for all words in the vocabulary :\n",
      " [1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718]\n",
      "\n",
      "TFIDF representation for \"geeks for geeks\" is \n",
      "[[0.         0.54935123 0.83559154 0.         0.         0.\n",
      "  0.        ]]\n",
      "TFIDF representation for \"geeks learning together fast\" is \n",
      "[[0.52863461 0.         0.40204024 0.52863461 0.         0.\n",
      "  0.52863461]]\n",
      "TFIDF representation for \"nlp ohe\" is \n",
      "[[0.         0.         0.         0.         0.70710678 0.70710678\n",
      "  0.        ]]\n",
      "\n",
      "TFIDF representation for 'learning dsa from AN' is [[0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# nltk.download('punkt') # Download 'punkt' \n",
    "# from nltk if it's not downloaded \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "Text = \"\"\"Geeks For Geeks. \n",
    "\t\tGeeks Learning Together Fast.\n",
    "        NLP OHE.\"\"\"\n",
    "\n",
    "# TOKENIZATION \n",
    "sentences = sent_tokenize(Text) \n",
    "sentences = [sent.lower().replace(\".\", \"\") for sent in sentences] \n",
    "print('\\nOur Corpus:', sentences) \n",
    "\n",
    "# TF-IDF \n",
    "tfidf = TfidfVectorizer() \n",
    "tfidf_matrix = tfidf.fit_transform(sentences) \n",
    "\n",
    "# All words in the vocabulary. \n",
    "print(\"\\nvocabulary:\", tfidf.get_feature_names_out()) \n",
    "# IDF value for all words in the vocabulary \n",
    "print(\"\\nIDF for all words in the vocabulary :\\n\", tfidf.idf_) \n",
    "\n",
    "# TFIDF representation for all documents in our corpus \n",
    "print('\\nTFIDF representation for \"{}\" is \\n{}'\n",
    "\t.format(sentences[0], tfidf_matrix[0].toarray())) \n",
    "print('TFIDF representation for \"{}\" is \\n{}'\n",
    "\t.format(sentences[1], tfidf_matrix[1].toarray())) \n",
    "print('TFIDF representation for \"{}\" is \\n{}'\n",
    "\t.format(sentences[2],tfidf_matrix[2].toarray())) \n",
    "\n",
    "# TFIDF representation for a new text \n",
    "matrix = tfidf.transform([\"learning dsa from AN\"]) \n",
    "print(\"\\nTFIDF representation for 'learning dsa from AN' is\", \n",
    "\tmatrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approach (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\Skill\\5. DataScience\\.venv\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\Skill\\5. DataScience\\.venv\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataprep 0.4.5 requires pydantic<2.0,>=1.6, but you have pydantic 2.10.4 which is incompatible.\n",
      "dataprep 0.4.5 requires sqlalchemy==1.3.24, but you have sqlalchemy 2.0.36 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in e:\\skill\\5. datascience\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\skill\\5. datascience\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in e:\\skill\\5. datascience\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 15.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.8/24.0 MB 14.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.9/24.0 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.5/24.0 MB 14.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.6/24.0 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.9/24.0 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.1/24.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.9/46.2 MB 14.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 14.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 9.2/46.2 MB 14.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 12.1/46.2 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 15.2/46.2 MB 14.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.8/46.2 MB 14.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 13.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 23.9/46.2 MB 14.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 26.5/46.2 MB 13.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.6/46.2 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 33.0/46.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.2/46.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.3/46.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.5/46.2 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.4/46.2 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api \n",
    "\n",
    "# load the pre-trained Word2Vec model \n",
    "# model = api.load('word2vec-google-news-300') \n",
    "\n",
    "# define word pairs to compute similarity for \n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')] \n",
    "\n",
    "# compute similarity for each pair of words \n",
    "for pair in word_pairs: \n",
    "\tsimilarity = model.similarity(pair[0], pair[1]) \n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n",
    "\n",
    "# # Output:\n",
    "# Similarity between 'learn' and 'learning' using Word2Vec: 0.637\n",
    "# Similarity between 'india' and 'indian' using Word2Vec: 0.697\n",
    "# Similarity between 'fame' and 'famous' using Word2Vec: 0.326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe by Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchtext.vocab as vocab \n",
    "\n",
    "# load the pre-trained GloVe model \n",
    "# glove = vocab.GloVe(name='840B', dim=300) \n",
    "\n",
    "# define word pairs to compute similarity for \n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')] \n",
    "\n",
    "# compute similarity for each pair of words \n",
    "for pair in word_pairs: \n",
    "\tvec1, vec2 = glove[pair[0]], glove[pair[1]] \n",
    "\tsimilarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2)) \n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n",
    "\n",
    "\n",
    "# # Output:\n",
    "# Similarity between 'learn' and 'learning' using GloVe: 0.768\n",
    "# Similarity between 'india' and 'indian' using GloVe: 0.764\n",
    "# Similarity between 'fame' and 'famous' using GloVe: 0.507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api \n",
    "\n",
    "# load the pre-trained fastText model \n",
    "# fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") \n",
    "\n",
    "# define word pairs to compute similarity for \n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')] \n",
    "\n",
    "# compute similarity for each pair of words \n",
    "for pair in word_pairs: \n",
    "\tsimilarity = fasttext_model.similarity(pair[0], pair[1]) \n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n",
    "\n",
    "\n",
    "# # Output:\n",
    "# Similarity between 'learn' and 'learning' using Word2Vec: 0.642\n",
    "# Similarity between 'india' and 'indian' using Word2Vec: 0.708\n",
    "# Similarity between 'fame' and 'famous' using Word2Vec: 0.519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
