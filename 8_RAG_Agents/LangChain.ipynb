{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Set Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain huggingface_hub langchain-community langchain-core sentence_transformers tf-keras -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install youtube-transcript-api pytube faiss-cpu wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models: Choosing from different LLMs and embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's pet is a parrot.\n"
     ]
    }
   ],
   "source": [
    "# # Proprietary LLM from e.g. OpenAI\n",
    "# # pip install openai\n",
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Alternatively, open-source LLM hosted on Hugging Face\n",
    "from langchain import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id = \"MBZUAI/LaMini-T5-738M\")\n",
    "\n",
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "prompt = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# Proprietary text embedding model from e.g. OpenAI\n",
    "# pip install tiktoken\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Alternatively, open-source text embedding model hosted on Hugging Face\n",
    "# pip install sentence_transformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# The embeddings model takes a text as an input and outputs a list of floats\n",
    "text = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print(len(text_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts: Managing LLM inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"What is a good name for a company that makes {product}?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give the antonym of every input\\n\\nWord: happy\\nAntonym: sad\\n\\n\\n\\nWord: tall\\nAntonym: short\\n\\n\\nWord: big\\nAntonym:'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "few_shot_prompt.format(input=\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains: Combining LLMs with other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['product'] input_types={} partial_variables={} template='What is a good name for a company that makes {product}?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product': 'colorful socks', 'text': '\"Colorful Socks\"'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "print(prompt)\n",
    "chain = LLMChain(llm = llm, \n",
    "                  prompt = prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "chain.invoke(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Colorful Socks\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Stay stylish with colorful socks.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Stay stylish with colorful socks.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Define the first chain as in the previous code example\n",
    "# ...\n",
    "\n",
    "# Create a second chain with a prompt template and an LLM\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"Write a catchphrase for the following company: {company_name}\",\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine the first and the second chain \n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"colorful socks\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes: Accessing external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
    "    \n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create the vectorestore to use as the index\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker is expressing their love for the other person and expressing their commitment to never let\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\",\n",
    "        search_kwargs={'k': 6,})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"What am I never going to do?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory: Remembering previous conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Alice has a parrot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Alice has a parrot.\n",
      "AI: AI: What is Alice's parrot's name? Human: It's Alice\n",
      "Human: Bob has two cats.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Alice has a parrot.\n",
      "AI: AI: What is Alice's parrot's name? Human: It's Alice\n",
      "Human: Bob has two cats.\n",
      "AI: The AI does not know the name of Alice's parrot.\n",
      "Human: How many pets do Alice and Bob have?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How many pets do Alice and Bob have?',\n",
       " 'history': \"Human: Alice has a parrot.\\nAI: AI: What is Alice's parrot's name? Human: It's Alice\\nHuman: Bob has two cats.\\nAI: The AI does not know the name of Alice's parrot.\",\n",
       " 'response': \"The AI does not know the name of Alice's parrot.\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.invoke(input=\"Alice has a parrot.\")\n",
    "\n",
    "conversation.invoke(input=\"Bob has two cats.\")\n",
    "\n",
    "conversation.invoke(input=\"How many pets do Alice and Bob have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents: Accessing other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n",
    "\n",
    "\n",
    "agent.run(\"When was Barack Obama born? How old was he in 2022?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
